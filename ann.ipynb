{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arathyjeevan/COVID-19/blob/master/ann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP4nsLhgEs6N"
      },
      "source": [
        "ANN CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE_T8ffG0Tg1",
        "outputId": "6f1a2c61-63a8-4d62-883c-e3374bc00119"
      },
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/major/pd.xls\")\n",
        "properties = list(df.columns.values)\n",
        "print(properties)\n",
        "properties.remove(17)\n",
        "X = df[properties]\n",
        "y = df[17]\n",
        "print(X)\n",
        "print(y)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
            "          1         2        3        4   ...      13       14       15       16\n",
            "0    0.00348  0.000024  0.00178  0.00191  ...  21.963  0.46413  0.57414  0.16807\n",
            "1    0.00097  0.000004  0.00009  0.00025  ...  11.965  0.59795  0.62956  0.75490\n",
            "2    0.00302  0.000013  0.00153  0.00141  ...  29.753  0.26885  0.63601  0.15521\n",
            "3    0.00774  0.000022  0.00194  0.00263  ...   3.415  0.59176  0.65600  0.78174\n",
            "4    0.00636  0.000034  0.00316  0.00332  ...  17.798  0.65999  0.64159  0.32376\n",
            "..       ...       ...      ...      ...  ...     ...      ...      ...      ...\n",
            "235  0.00814  0.000052  0.00334  0.00404  ...  17.069  0.62887  0.75147  0.33377\n",
            "236  0.00571  0.000032  0.00309  0.00279  ...  23.226  0.51443  0.66074  0.19073\n",
            "237  0.00294  0.000018  0.00050  0.00127  ...  17.277  0.65426  0.69168  0.82140\n",
            "238  0.07183  0.000364  0.04356  0.04744  ...   2.206  0.82670  0.63404  0.43811\n",
            "239  0.00580  0.000041  0.00276  0.00312  ...  21.423  0.45793  0.66908  0.21821\n",
            "\n",
            "[240 rows x 16 columns]\n",
            "0      1\n",
            "1      0\n",
            "2      1\n",
            "3      0\n",
            "4      1\n",
            "      ..\n",
            "235    1\n",
            "236    1\n",
            "237    0\n",
            "238    1\n",
            "239    1\n",
            "Name: 17, Length: 240, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at2mwmAn0anJ",
        "outputId": "a269d81e-7953-4dc1-b858-e92b6f147ff9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(16,)),\n",
        "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
        "\tkeras.layers.Dense(16, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=1)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "168/168 [==============================] - 0s 861us/step - loss: 0.6977 - accuracy: 0.5595\n",
            "Epoch 2/150\n",
            "168/168 [==============================] - 0s 842us/step - loss: 0.7018 - accuracy: 0.4940\n",
            "Epoch 3/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.6747 - accuracy: 0.5774\n",
            "Epoch 4/150\n",
            "168/168 [==============================] - 0s 840us/step - loss: 0.6668 - accuracy: 0.5952\n",
            "Epoch 5/150\n",
            "168/168 [==============================] - 0s 841us/step - loss: 0.6513 - accuracy: 0.6964\n",
            "Epoch 6/150\n",
            "168/168 [==============================] - 0s 865us/step - loss: 0.6547 - accuracy: 0.6726\n",
            "Epoch 7/150\n",
            "168/168 [==============================] - 0s 854us/step - loss: 0.6564 - accuracy: 0.6071\n",
            "Epoch 8/150\n",
            "168/168 [==============================] - 0s 871us/step - loss: 0.6123 - accuracy: 0.6726\n",
            "Epoch 9/150\n",
            "168/168 [==============================] - 0s 842us/step - loss: 0.6048 - accuracy: 0.7679\n",
            "Epoch 10/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.5860 - accuracy: 0.7500\n",
            "Epoch 11/150\n",
            "168/168 [==============================] - 0s 965us/step - loss: 0.5736 - accuracy: 0.7857\n",
            "Epoch 12/150\n",
            "168/168 [==============================] - 0s 854us/step - loss: 0.5570 - accuracy: 0.7857\n",
            "Epoch 13/150\n",
            "168/168 [==============================] - 0s 890us/step - loss: 0.5494 - accuracy: 0.7976\n",
            "Epoch 14/150\n",
            "168/168 [==============================] - 0s 880us/step - loss: 0.5164 - accuracy: 0.7857\n",
            "Epoch 15/150\n",
            "168/168 [==============================] - 0s 850us/step - loss: 0.5269 - accuracy: 0.8095\n",
            "Epoch 16/150\n",
            "168/168 [==============================] - 0s 966us/step - loss: 0.5093 - accuracy: 0.8214\n",
            "Epoch 17/150\n",
            "168/168 [==============================] - 0s 856us/step - loss: 0.4960 - accuracy: 0.8214\n",
            "Epoch 18/150\n",
            "168/168 [==============================] - 0s 948us/step - loss: 0.4896 - accuracy: 0.8095\n",
            "Epoch 19/150\n",
            "168/168 [==============================] - 0s 862us/step - loss: 0.4840 - accuracy: 0.7976\n",
            "Epoch 20/150\n",
            "168/168 [==============================] - 0s 909us/step - loss: 0.4757 - accuracy: 0.8333\n",
            "Epoch 21/150\n",
            "168/168 [==============================] - 0s 959us/step - loss: 0.4561 - accuracy: 0.8333\n",
            "Epoch 22/150\n",
            "168/168 [==============================] - 0s 878us/step - loss: 0.4557 - accuracy: 0.8214\n",
            "Epoch 23/150\n",
            "168/168 [==============================] - 0s 952us/step - loss: 0.4522 - accuracy: 0.8333\n",
            "Epoch 24/150\n",
            "168/168 [==============================] - 0s 864us/step - loss: 0.4603 - accuracy: 0.8333\n",
            "Epoch 25/150\n",
            "168/168 [==============================] - 0s 859us/step - loss: 0.4448 - accuracy: 0.8393\n",
            "Epoch 26/150\n",
            "168/168 [==============================] - 0s 877us/step - loss: 0.4393 - accuracy: 0.8512\n",
            "Epoch 27/150\n",
            "168/168 [==============================] - 0s 837us/step - loss: 0.4530 - accuracy: 0.8155\n",
            "Epoch 28/150\n",
            "168/168 [==============================] - 0s 892us/step - loss: 0.4444 - accuracy: 0.8393\n",
            "Epoch 29/150\n",
            "168/168 [==============================] - 0s 963us/step - loss: 0.4337 - accuracy: 0.8393\n",
            "Epoch 30/150\n",
            "168/168 [==============================] - 0s 881us/step - loss: 0.4400 - accuracy: 0.8452\n",
            "Epoch 31/150\n",
            "168/168 [==============================] - 0s 933us/step - loss: 0.4414 - accuracy: 0.8333\n",
            "Epoch 32/150\n",
            "168/168 [==============================] - 0s 886us/step - loss: 0.4383 - accuracy: 0.8512\n",
            "Epoch 33/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.8452\n",
            "Epoch 34/150\n",
            "168/168 [==============================] - 0s 988us/step - loss: 0.4294 - accuracy: 0.8333\n",
            "Epoch 35/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8333\n",
            "Epoch 36/150\n",
            "168/168 [==============================] - 0s 888us/step - loss: 0.4372 - accuracy: 0.8452\n",
            "Epoch 37/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.4427 - accuracy: 0.8333\n",
            "Epoch 38/150\n",
            "168/168 [==============================] - 0s 847us/step - loss: 0.4400 - accuracy: 0.8333\n",
            "Epoch 39/150\n",
            "168/168 [==============================] - 0s 963us/step - loss: 0.4252 - accuracy: 0.8333\n",
            "Epoch 40/150\n",
            "168/168 [==============================] - 0s 882us/step - loss: 0.4222 - accuracy: 0.8393\n",
            "Epoch 41/150\n",
            "168/168 [==============================] - 0s 890us/step - loss: 0.4261 - accuracy: 0.8333\n",
            "Epoch 42/150\n",
            "168/168 [==============================] - 0s 935us/step - loss: 0.4170 - accuracy: 0.8393\n",
            "Epoch 43/150\n",
            "168/168 [==============================] - 0s 944us/step - loss: 0.4218 - accuracy: 0.8333\n",
            "Epoch 44/150\n",
            "168/168 [==============================] - 0s 872us/step - loss: 0.4242 - accuracy: 0.8333\n",
            "Epoch 45/150\n",
            "168/168 [==============================] - 0s 873us/step - loss: 0.4131 - accuracy: 0.8452\n",
            "Epoch 46/150\n",
            "168/168 [==============================] - 0s 970us/step - loss: 0.4203 - accuracy: 0.8393\n",
            "Epoch 47/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.8571\n",
            "Epoch 48/150\n",
            "168/168 [==============================] - 0s 959us/step - loss: 0.4175 - accuracy: 0.8452\n",
            "Epoch 49/150\n",
            "168/168 [==============================] - 0s 882us/step - loss: 0.4137 - accuracy: 0.8274\n",
            "Epoch 50/150\n",
            "168/168 [==============================] - 0s 886us/step - loss: 0.4214 - accuracy: 0.8393\n",
            "Epoch 51/150\n",
            "168/168 [==============================] - 0s 907us/step - loss: 0.4014 - accuracy: 0.8512\n",
            "Epoch 52/150\n",
            "168/168 [==============================] - 0s 949us/step - loss: 0.4312 - accuracy: 0.8333\n",
            "Epoch 53/150\n",
            "168/168 [==============================] - 0s 909us/step - loss: 0.4123 - accuracy: 0.8393\n",
            "Epoch 54/150\n",
            "168/168 [==============================] - 0s 928us/step - loss: 0.4021 - accuracy: 0.8393\n",
            "Epoch 55/150\n",
            "168/168 [==============================] - 0s 875us/step - loss: 0.4026 - accuracy: 0.8393\n",
            "Epoch 56/150\n",
            "168/168 [==============================] - 0s 956us/step - loss: 0.4026 - accuracy: 0.8333\n",
            "Epoch 57/150\n",
            "168/168 [==============================] - 0s 873us/step - loss: 0.4027 - accuracy: 0.8452\n",
            "Epoch 58/150\n",
            "168/168 [==============================] - 0s 886us/step - loss: 0.4135 - accuracy: 0.8452\n",
            "Epoch 59/150\n",
            "168/168 [==============================] - 0s 873us/step - loss: 0.4144 - accuracy: 0.8333\n",
            "Epoch 60/150\n",
            "168/168 [==============================] - 0s 845us/step - loss: 0.4020 - accuracy: 0.8393\n",
            "Epoch 61/150\n",
            "168/168 [==============================] - 0s 897us/step - loss: 0.4091 - accuracy: 0.8393\n",
            "Epoch 62/150\n",
            "168/168 [==============================] - 0s 927us/step - loss: 0.4031 - accuracy: 0.8274\n",
            "Epoch 63/150\n",
            "168/168 [==============================] - 0s 857us/step - loss: 0.4041 - accuracy: 0.8393\n",
            "Epoch 64/150\n",
            "168/168 [==============================] - 0s 911us/step - loss: 0.3949 - accuracy: 0.8512\n",
            "Epoch 65/150\n",
            "168/168 [==============================] - 0s 862us/step - loss: 0.3892 - accuracy: 0.8393\n",
            "Epoch 66/150\n",
            "168/168 [==============================] - 0s 850us/step - loss: 0.3973 - accuracy: 0.8333\n",
            "Epoch 67/150\n",
            "168/168 [==============================] - 0s 872us/step - loss: 0.3909 - accuracy: 0.8452\n",
            "Epoch 68/150\n",
            "168/168 [==============================] - 0s 865us/step - loss: 0.3852 - accuracy: 0.8631\n",
            "Epoch 69/150\n",
            "168/168 [==============================] - 0s 868us/step - loss: 0.3868 - accuracy: 0.8333\n",
            "Epoch 70/150\n",
            "168/168 [==============================] - 0s 888us/step - loss: 0.4062 - accuracy: 0.8214\n",
            "Epoch 71/150\n",
            "168/168 [==============================] - 0s 900us/step - loss: 0.3921 - accuracy: 0.8571\n",
            "Epoch 72/150\n",
            "168/168 [==============================] - 0s 979us/step - loss: 0.3944 - accuracy: 0.8214\n",
            "Epoch 73/150\n",
            "168/168 [==============================] - 0s 881us/step - loss: 0.3990 - accuracy: 0.8452\n",
            "Epoch 74/150\n",
            "168/168 [==============================] - 0s 905us/step - loss: 0.3929 - accuracy: 0.8512\n",
            "Epoch 75/150\n",
            "168/168 [==============================] - 0s 893us/step - loss: 0.3855 - accuracy: 0.8452\n",
            "Epoch 76/150\n",
            "168/168 [==============================] - 0s 876us/step - loss: 0.3972 - accuracy: 0.8333\n",
            "Epoch 77/150\n",
            "168/168 [==============================] - 0s 959us/step - loss: 0.3888 - accuracy: 0.8452\n",
            "Epoch 78/150\n",
            "168/168 [==============================] - 0s 975us/step - loss: 0.4045 - accuracy: 0.8155\n",
            "Epoch 79/150\n",
            "168/168 [==============================] - 0s 872us/step - loss: 0.3814 - accuracy: 0.8274\n",
            "Epoch 80/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3965 - accuracy: 0.8333\n",
            "Epoch 81/150\n",
            "168/168 [==============================] - 0s 932us/step - loss: 0.3866 - accuracy: 0.8333\n",
            "Epoch 82/150\n",
            "168/168 [==============================] - 0s 993us/step - loss: 0.3877 - accuracy: 0.8393\n",
            "Epoch 83/150\n",
            "168/168 [==============================] - 0s 876us/step - loss: 0.3848 - accuracy: 0.8452\n",
            "Epoch 84/150\n",
            "168/168 [==============================] - 0s 910us/step - loss: 0.3826 - accuracy: 0.8512\n",
            "Epoch 85/150\n",
            "168/168 [==============================] - 0s 891us/step - loss: 0.3833 - accuracy: 0.8452\n",
            "Epoch 86/150\n",
            "168/168 [==============================] - 0s 871us/step - loss: 0.3743 - accuracy: 0.8452\n",
            "Epoch 87/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8512\n",
            "Epoch 88/150\n",
            "168/168 [==============================] - 0s 850us/step - loss: 0.3861 - accuracy: 0.8393\n",
            "Epoch 89/150\n",
            "168/168 [==============================] - 0s 887us/step - loss: 0.3987 - accuracy: 0.8393\n",
            "Epoch 90/150\n",
            "168/168 [==============================] - 0s 859us/step - loss: 0.4011 - accuracy: 0.8214\n",
            "Epoch 91/150\n",
            "168/168 [==============================] - 0s 959us/step - loss: 0.3826 - accuracy: 0.8512\n",
            "Epoch 92/150\n",
            "168/168 [==============================] - 0s 897us/step - loss: 0.3892 - accuracy: 0.8393\n",
            "Epoch 93/150\n",
            "168/168 [==============================] - 0s 932us/step - loss: 0.3874 - accuracy: 0.8512\n",
            "Epoch 94/150\n",
            "168/168 [==============================] - 0s 882us/step - loss: 0.3889 - accuracy: 0.8393\n",
            "Epoch 95/150\n",
            "168/168 [==============================] - 0s 857us/step - loss: 0.3739 - accuracy: 0.8393\n",
            "Epoch 96/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8393\n",
            "Epoch 97/150\n",
            "168/168 [==============================] - 0s 925us/step - loss: 0.3846 - accuracy: 0.8512\n",
            "Epoch 98/150\n",
            "168/168 [==============================] - 0s 888us/step - loss: 0.3911 - accuracy: 0.8333\n",
            "Epoch 99/150\n",
            "168/168 [==============================] - 0s 986us/step - loss: 0.3829 - accuracy: 0.8631\n",
            "Epoch 100/150\n",
            "168/168 [==============================] - 0s 879us/step - loss: 0.3751 - accuracy: 0.8452\n",
            "Epoch 101/150\n",
            "168/168 [==============================] - 0s 875us/step - loss: 0.3751 - accuracy: 0.8571\n",
            "Epoch 102/150\n",
            "168/168 [==============================] - 0s 896us/step - loss: 0.3778 - accuracy: 0.8333\n",
            "Epoch 103/150\n",
            "168/168 [==============================] - 0s 892us/step - loss: 0.3718 - accuracy: 0.8393\n",
            "Epoch 104/150\n",
            "168/168 [==============================] - 0s 950us/step - loss: 0.3770 - accuracy: 0.8333\n",
            "Epoch 105/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8155\n",
            "Epoch 106/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3724 - accuracy: 0.8274\n",
            "Epoch 107/150\n",
            "168/168 [==============================] - 0s 891us/step - loss: 0.3773 - accuracy: 0.8393\n",
            "Epoch 108/150\n",
            "168/168 [==============================] - 0s 952us/step - loss: 0.3719 - accuracy: 0.8631\n",
            "Epoch 109/150\n",
            "168/168 [==============================] - 0s 983us/step - loss: 0.3656 - accuracy: 0.8393\n",
            "Epoch 110/150\n",
            "168/168 [==============================] - 0s 841us/step - loss: 0.3782 - accuracy: 0.8333\n",
            "Epoch 111/150\n",
            "168/168 [==============================] - 0s 972us/step - loss: 0.3812 - accuracy: 0.8452\n",
            "Epoch 112/150\n",
            "168/168 [==============================] - 0s 989us/step - loss: 0.3696 - accuracy: 0.8452\n",
            "Epoch 113/150\n",
            "168/168 [==============================] - 0s 956us/step - loss: 0.3755 - accuracy: 0.8452\n",
            "Epoch 114/150\n",
            "168/168 [==============================] - 0s 891us/step - loss: 0.3721 - accuracy: 0.8155\n",
            "Epoch 115/150\n",
            "168/168 [==============================] - 0s 954us/step - loss: 0.3713 - accuracy: 0.8512\n",
            "Epoch 116/150\n",
            "168/168 [==============================] - 0s 929us/step - loss: 0.3792 - accuracy: 0.8512\n",
            "Epoch 117/150\n",
            "168/168 [==============================] - 0s 882us/step - loss: 0.3789 - accuracy: 0.8393\n",
            "Epoch 118/150\n",
            "168/168 [==============================] - 0s 928us/step - loss: 0.3703 - accuracy: 0.8452\n",
            "Epoch 119/150\n",
            "168/168 [==============================] - 0s 967us/step - loss: 0.3733 - accuracy: 0.8452\n",
            "Epoch 120/150\n",
            "168/168 [==============================] - 0s 856us/step - loss: 0.3665 - accuracy: 0.8333\n",
            "Epoch 121/150\n",
            "168/168 [==============================] - 0s 894us/step - loss: 0.3729 - accuracy: 0.8452\n",
            "Epoch 122/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.8452\n",
            "Epoch 123/150\n",
            "168/168 [==============================] - 0s 897us/step - loss: 0.3658 - accuracy: 0.8452\n",
            "Epoch 124/150\n",
            "168/168 [==============================] - 0s 968us/step - loss: 0.3706 - accuracy: 0.8214\n",
            "Epoch 125/150\n",
            "168/168 [==============================] - 0s 926us/step - loss: 0.3651 - accuracy: 0.8393\n",
            "Epoch 126/150\n",
            "168/168 [==============================] - 0s 955us/step - loss: 0.3479 - accuracy: 0.8690\n",
            "Epoch 127/150\n",
            "168/168 [==============================] - 0s 948us/step - loss: 0.3749 - accuracy: 0.8274\n",
            "Epoch 128/150\n",
            "168/168 [==============================] - 0s 906us/step - loss: 0.3648 - accuracy: 0.8274\n",
            "Epoch 129/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8631\n",
            "Epoch 130/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3523 - accuracy: 0.8571\n",
            "Epoch 131/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.8333\n",
            "Epoch 132/150\n",
            "168/168 [==============================] - 0s 900us/step - loss: 0.3589 - accuracy: 0.8512\n",
            "Epoch 133/150\n",
            "168/168 [==============================] - 0s 887us/step - loss: 0.3595 - accuracy: 0.8631\n",
            "Epoch 134/150\n",
            "168/168 [==============================] - 0s 979us/step - loss: 0.3633 - accuracy: 0.8571\n",
            "Epoch 135/150\n",
            "168/168 [==============================] - 0s 885us/step - loss: 0.3545 - accuracy: 0.8452\n",
            "Epoch 136/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3752 - accuracy: 0.8452\n",
            "Epoch 137/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3665 - accuracy: 0.8571\n",
            "Epoch 138/150\n",
            "168/168 [==============================] - 0s 985us/step - loss: 0.3581 - accuracy: 0.8274\n",
            "Epoch 139/150\n",
            "168/168 [==============================] - 0s 966us/step - loss: 0.3542 - accuracy: 0.8512\n",
            "Epoch 140/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3504 - accuracy: 0.8393\n",
            "Epoch 141/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8333\n",
            "Epoch 142/150\n",
            "168/168 [==============================] - 0s 972us/step - loss: 0.3580 - accuracy: 0.8393\n",
            "Epoch 143/150\n",
            "168/168 [==============================] - 0s 959us/step - loss: 0.3644 - accuracy: 0.8452\n",
            "Epoch 144/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8274\n",
            "Epoch 145/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8274\n",
            "Epoch 146/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.3509 - accuracy: 0.8690\n",
            "Epoch 147/150\n",
            "168/168 [==============================] - 0s 915us/step - loss: 0.3594 - accuracy: 0.8452\n",
            "Epoch 148/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3576 - accuracy: 0.8631\n",
            "Epoch 149/150\n",
            "168/168 [==============================] - 0s 887us/step - loss: 0.3644 - accuracy: 0.8214\n",
            "Epoch 150/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8333\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8611\n",
            "Test accuracy: 0.8611111044883728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuxFD3xF1nYk",
        "outputId": "0b47a2bf-6e87-404f-f53a-fb822fd426e7"
      },
      "source": [
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(168, 16) (72, 16) (168,) (72,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPP77KLiJbz8",
        "outputId": "c219babe-06ef-426b-f1d1-8b213c2a4058"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJOcdTz298h5",
        "outputId": "f8afc116-8317-4530-b989-2f7c19f819cd"
      },
      "source": [
        "model.evaluate(X_test, y_test)[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8611111044883728"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUjiYEwH-8Q6",
        "outputId": "30b066af-05be-4561-b066-265118b4e3de"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_96\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_157 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 561\n",
            "Trainable params: 561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiY-_7yZ_far",
        "outputId": "f4fe487a-dbfe-4a47-852c-ca30e1586f81"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "print(predictions.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(72, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGZvEN0DJRwV",
        "outputId": "05da90c3-b705-41f5-bf30-66276584f70f"
      },
      "source": [
        "tf.keras.metrics.FalsePositives(\n",
        "    thresholds=None, name=None, dtype=None\n",
        ")\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "               metrics=[tf.keras.metrics.FalsePositives()])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=1)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('False Positives:', test_acc)\n",
        "FP=test_acc"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "168/168 [==============================] - 0s 930us/step - loss: 0.2568 - false_positives_5: 13.0000\n",
            "Epoch 2/150\n",
            "168/168 [==============================] - 0s 961us/step - loss: 0.2426 - false_positives_5: 10.0000\n",
            "Epoch 3/150\n",
            "168/168 [==============================] - 0s 879us/step - loss: 0.2509 - false_positives_5: 9.0000\n",
            "Epoch 4/150\n",
            "168/168 [==============================] - 0s 946us/step - loss: 0.2414 - false_positives_5: 12.0000\n",
            "Epoch 5/150\n",
            "168/168 [==============================] - 0s 933us/step - loss: 0.2418 - false_positives_5: 10.0000\n",
            "Epoch 6/150\n",
            "168/168 [==============================] - 0s 881us/step - loss: 0.2360 - false_positives_5: 10.0000\n",
            "Epoch 7/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2455 - false_positives_5: 11.0000\n",
            "Epoch 8/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2421 - false_positives_5: 10.0000\n",
            "Epoch 9/150\n",
            "168/168 [==============================] - 0s 883us/step - loss: 0.2477 - false_positives_5: 9.0000\n",
            "Epoch 10/150\n",
            "168/168 [==============================] - 0s 931us/step - loss: 0.2321 - false_positives_5: 12.0000\n",
            "Epoch 11/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2350 - false_positives_5: 12.0000\n",
            "Epoch 12/150\n",
            "168/168 [==============================] - 0s 896us/step - loss: 0.2410 - false_positives_5: 10.0000\n",
            "Epoch 13/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2643 - false_positives_5: 13.0000\n",
            "Epoch 14/150\n",
            "168/168 [==============================] - 0s 948us/step - loss: 0.2475 - false_positives_5: 11.0000\n",
            "Epoch 15/150\n",
            "168/168 [==============================] - 0s 956us/step - loss: 0.2700 - false_positives_5: 12.0000\n",
            "Epoch 16/150\n",
            "168/168 [==============================] - 0s 889us/step - loss: 0.2462 - false_positives_5: 11.0000\n",
            "Epoch 17/150\n",
            "168/168 [==============================] - 0s 982us/step - loss: 0.2548 - false_positives_5: 12.0000\n",
            "Epoch 18/150\n",
            "168/168 [==============================] - 0s 950us/step - loss: 0.2482 - false_positives_5: 11.0000\n",
            "Epoch 19/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2293 - false_positives_5: 9.0000\n",
            "Epoch 20/150\n",
            "168/168 [==============================] - 0s 989us/step - loss: 0.2434 - false_positives_5: 10.0000\n",
            "Epoch 21/150\n",
            "168/168 [==============================] - 0s 874us/step - loss: 0.2350 - false_positives_5: 12.0000\n",
            "Epoch 22/150\n",
            "168/168 [==============================] - 0s 895us/step - loss: 0.2181 - false_positives_5: 9.0000\n",
            "Epoch 23/150\n",
            "168/168 [==============================] - 0s 979us/step - loss: 0.2226 - false_positives_5: 12.0000\n",
            "Epoch 24/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2436 - false_positives_5: 12.0000\n",
            "Epoch 25/150\n",
            "168/168 [==============================] - 0s 976us/step - loss: 0.2645 - false_positives_5: 11.0000\n",
            "Epoch 26/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2683 - false_positives_5: 11.0000\n",
            "Epoch 27/150\n",
            "168/168 [==============================] - 0s 968us/step - loss: 0.2505 - false_positives_5: 12.0000\n",
            "Epoch 28/150\n",
            "168/168 [==============================] - 0s 945us/step - loss: 0.2271 - false_positives_5: 9.0000\n",
            "Epoch 29/150\n",
            "168/168 [==============================] - 0s 954us/step - loss: 0.2544 - false_positives_5: 10.0000\n",
            "Epoch 30/150\n",
            "168/168 [==============================] - 0s 963us/step - loss: 0.2429 - false_positives_5: 8.0000\n",
            "Epoch 31/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2528 - false_positives_5: 12.0000\n",
            "Epoch 32/150\n",
            "168/168 [==============================] - 0s 945us/step - loss: 0.2461 - false_positives_5: 11.0000\n",
            "Epoch 33/150\n",
            "168/168 [==============================] - 0s 947us/step - loss: 0.2312 - false_positives_5: 9.0000\n",
            "Epoch 34/150\n",
            "168/168 [==============================] - 0s 891us/step - loss: 0.2389 - false_positives_5: 10.0000\n",
            "Epoch 35/150\n",
            "168/168 [==============================] - 0s 948us/step - loss: 0.2467 - false_positives_5: 12.0000\n",
            "Epoch 36/150\n",
            "168/168 [==============================] - 0s 950us/step - loss: 0.2407 - false_positives_5: 10.0000\n",
            "Epoch 37/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2463 - false_positives_5: 11.0000\n",
            "Epoch 38/150\n",
            "168/168 [==============================] - 0s 942us/step - loss: 0.2384 - false_positives_5: 8.0000\n",
            "Epoch 39/150\n",
            "168/168 [==============================] - 0s 940us/step - loss: 0.2271 - false_positives_5: 10.0000\n",
            "Epoch 40/150\n",
            "168/168 [==============================] - 0s 1000us/step - loss: 0.2456 - false_positives_5: 8.0000\n",
            "Epoch 41/150\n",
            "168/168 [==============================] - 0s 995us/step - loss: 0.2346 - false_positives_5: 11.0000\n",
            "Epoch 42/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2258 - false_positives_5: 10.0000\n",
            "Epoch 43/150\n",
            "168/168 [==============================] - 0s 976us/step - loss: 0.2437 - false_positives_5: 9.0000\n",
            "Epoch 44/150\n",
            "168/168 [==============================] - 0s 885us/step - loss: 0.2378 - false_positives_5: 11.0000\n",
            "Epoch 45/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2315 - false_positives_5: 9.0000\n",
            "Epoch 46/150\n",
            "168/168 [==============================] - 0s 890us/step - loss: 0.2118 - false_positives_5: 9.0000\n",
            "Epoch 47/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2421 - false_positives_5: 7.0000\n",
            "Epoch 48/150\n",
            "168/168 [==============================] - 0s 888us/step - loss: 0.2229 - false_positives_5: 9.0000\n",
            "Epoch 49/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2534 - false_positives_5: 11.0000\n",
            "Epoch 50/150\n",
            "168/168 [==============================] - 0s 970us/step - loss: 0.2518 - false_positives_5: 10.0000\n",
            "Epoch 51/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2562 - false_positives_5: 14.0000\n",
            "Epoch 52/150\n",
            "168/168 [==============================] - 0s 965us/step - loss: 0.2470 - false_positives_5: 13.0000\n",
            "Epoch 53/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2366 - false_positives_5: 10.0000\n",
            "Epoch 54/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2389 - false_positives_5: 11.0000\n",
            "Epoch 55/150\n",
            "168/168 [==============================] - 0s 996us/step - loss: 0.2640 - false_positives_5: 11.0000\n",
            "Epoch 56/150\n",
            "168/168 [==============================] - 0s 968us/step - loss: 0.2338 - false_positives_5: 10.0000\n",
            "Epoch 57/150\n",
            "168/168 [==============================] - 0s 970us/step - loss: 0.2508 - false_positives_5: 9.0000\n",
            "Epoch 58/150\n",
            "168/168 [==============================] - 0s 954us/step - loss: 0.2320 - false_positives_5: 7.0000\n",
            "Epoch 59/150\n",
            "168/168 [==============================] - 0s 991us/step - loss: 0.2261 - false_positives_5: 9.0000\n",
            "Epoch 60/150\n",
            "168/168 [==============================] - 0s 898us/step - loss: 0.2321 - false_positives_5: 10.0000\n",
            "Epoch 61/150\n",
            "168/168 [==============================] - 0s 895us/step - loss: 0.2387 - false_positives_5: 11.0000\n",
            "Epoch 62/150\n",
            "168/168 [==============================] - 0s 893us/step - loss: 0.2316 - false_positives_5: 8.0000\n",
            "Epoch 63/150\n",
            "168/168 [==============================] - 0s 996us/step - loss: 0.2400 - false_positives_5: 10.0000\n",
            "Epoch 64/150\n",
            "168/168 [==============================] - 0s 994us/step - loss: 0.2369 - false_positives_5: 13.0000\n",
            "Epoch 65/150\n",
            "168/168 [==============================] - 0s 961us/step - loss: 0.2575 - false_positives_5: 10.0000\n",
            "Epoch 66/150\n",
            "168/168 [==============================] - 0s 957us/step - loss: 0.2226 - false_positives_5: 7.0000\n",
            "Epoch 67/150\n",
            "168/168 [==============================] - 0s 944us/step - loss: 0.2412 - false_positives_5: 10.0000\n",
            "Epoch 68/150\n",
            "168/168 [==============================] - 0s 935us/step - loss: 0.2070 - false_positives_5: 9.0000\n",
            "Epoch 69/150\n",
            "168/168 [==============================] - 0s 966us/step - loss: 0.2366 - false_positives_5: 11.0000\n",
            "Epoch 70/150\n",
            "168/168 [==============================] - 0s 969us/step - loss: 0.2297 - false_positives_5: 10.0000\n",
            "Epoch 71/150\n",
            "168/168 [==============================] - 0s 992us/step - loss: 0.2296 - false_positives_5: 10.0000\n",
            "Epoch 72/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2310 - false_positives_5: 10.0000\n",
            "Epoch 73/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2172 - false_positives_5: 12.0000\n",
            "Epoch 74/150\n",
            "168/168 [==============================] - 0s 883us/step - loss: 0.2376 - false_positives_5: 9.0000\n",
            "Epoch 75/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2202 - false_positives_5: 8.0000\n",
            "Epoch 76/150\n",
            "168/168 [==============================] - 0s 998us/step - loss: 0.2192 - false_positives_5: 9.0000\n",
            "Epoch 77/150\n",
            "168/168 [==============================] - 0s 964us/step - loss: 0.2266 - false_positives_5: 7.0000\n",
            "Epoch 78/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2427 - false_positives_5: 9.0000\n",
            "Epoch 79/150\n",
            "168/168 [==============================] - 0s 953us/step - loss: 0.2263 - false_positives_5: 12.0000\n",
            "Epoch 80/150\n",
            "168/168 [==============================] - 0s 960us/step - loss: 0.2483 - false_positives_5: 9.0000\n",
            "Epoch 81/150\n",
            "168/168 [==============================] - 0s 948us/step - loss: 0.2424 - false_positives_5: 11.0000\n",
            "Epoch 82/150\n",
            "168/168 [==============================] - 0s 895us/step - loss: 0.2338 - false_positives_5: 9.0000\n",
            "Epoch 83/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2500 - false_positives_5: 11.0000\n",
            "Epoch 84/150\n",
            "168/168 [==============================] - 0s 895us/step - loss: 0.2309 - false_positives_5: 8.0000\n",
            "Epoch 85/150\n",
            "168/168 [==============================] - 0s 917us/step - loss: 0.2292 - false_positives_5: 8.0000\n",
            "Epoch 86/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2384 - false_positives_5: 11.0000\n",
            "Epoch 87/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2281 - false_positives_5: 11.0000\n",
            "Epoch 88/150\n",
            "168/168 [==============================] - 0s 945us/step - loss: 0.2306 - false_positives_5: 12.0000\n",
            "Epoch 89/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2568 - false_positives_5: 8.0000\n",
            "Epoch 90/150\n",
            "168/168 [==============================] - 0s 969us/step - loss: 0.2311 - false_positives_5: 9.0000\n",
            "Epoch 91/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2331 - false_positives_5: 9.0000\n",
            "Epoch 92/150\n",
            "168/168 [==============================] - 0s 954us/step - loss: 0.2136 - false_positives_5: 8.0000\n",
            "Epoch 93/150\n",
            "168/168 [==============================] - 0s 968us/step - loss: 0.2325 - false_positives_5: 7.0000\n",
            "Epoch 94/150\n",
            "168/168 [==============================] - 0s 940us/step - loss: 0.2536 - false_positives_5: 12.0000\n",
            "Epoch 95/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2245 - false_positives_5: 9.0000\n",
            "Epoch 96/150\n",
            "168/168 [==============================] - 0s 991us/step - loss: 0.2417 - false_positives_5: 8.0000\n",
            "Epoch 97/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2411 - false_positives_5: 12.0000\n",
            "Epoch 98/150\n",
            "168/168 [==============================] - 0s 987us/step - loss: 0.2191 - false_positives_5: 7.0000\n",
            "Epoch 99/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2236 - false_positives_5: 8.0000\n",
            "Epoch 100/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2948 - false_positives_5: 11.0000\n",
            "Epoch 101/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2425 - false_positives_5: 10.0000\n",
            "Epoch 102/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2169 - false_positives_5: 9.0000\n",
            "Epoch 103/150\n",
            "168/168 [==============================] - 0s 980us/step - loss: 0.2262 - false_positives_5: 10.0000\n",
            "Epoch 104/150\n",
            "168/168 [==============================] - 0s 952us/step - loss: 0.2251 - false_positives_5: 9.0000\n",
            "Epoch 105/150\n",
            "168/168 [==============================] - 0s 978us/step - loss: 0.2379 - false_positives_5: 8.0000\n",
            "Epoch 106/150\n",
            "168/168 [==============================] - 0s 920us/step - loss: 0.2161 - false_positives_5: 9.0000\n",
            "Epoch 107/150\n",
            "168/168 [==============================] - 0s 984us/step - loss: 0.2328 - false_positives_5: 11.0000\n",
            "Epoch 108/150\n",
            "168/168 [==============================] - 0s 944us/step - loss: 0.2379 - false_positives_5: 11.0000\n",
            "Epoch 109/150\n",
            "168/168 [==============================] - 0s 936us/step - loss: 0.2119 - false_positives_5: 9.0000\n",
            "Epoch 110/150\n",
            "168/168 [==============================] - 0s 954us/step - loss: 0.2252 - false_positives_5: 8.0000\n",
            "Epoch 111/150\n",
            "168/168 [==============================] - 0s 907us/step - loss: 0.2222 - false_positives_5: 8.0000\n",
            "Epoch 112/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2104 - false_positives_5: 8.0000\n",
            "Epoch 113/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2455 - false_positives_5: 10.0000\n",
            "Epoch 114/150\n",
            "168/168 [==============================] - 0s 929us/step - loss: 0.2448 - false_positives_5: 10.0000\n",
            "Epoch 115/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2385 - false_positives_5: 9.0000\n",
            "Epoch 116/150\n",
            "168/168 [==============================] - 0s 885us/step - loss: 0.2339 - false_positives_5: 9.0000\n",
            "Epoch 117/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2368 - false_positives_5: 11.0000\n",
            "Epoch 118/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2327 - false_positives_5: 9.0000\n",
            "Epoch 119/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.2178 - false_positives_5: 6.0000\n",
            "Epoch 120/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2339 - false_positives_5: 8.0000\n",
            "Epoch 121/150\n",
            "168/168 [==============================] - 0s 973us/step - loss: 0.2338 - false_positives_5: 11.0000\n",
            "Epoch 122/150\n",
            "168/168 [==============================] - 0s 957us/step - loss: 0.2199 - false_positives_5: 9.0000\n",
            "Epoch 123/150\n",
            "168/168 [==============================] - 0s 982us/step - loss: 0.2217 - false_positives_5: 8.0000\n",
            "Epoch 124/150\n",
            "168/168 [==============================] - 0s 967us/step - loss: 0.2273 - false_positives_5: 6.0000\n",
            "Epoch 125/150\n",
            "168/168 [==============================] - 0s 961us/step - loss: 0.2256 - false_positives_5: 8.0000\n",
            "Epoch 126/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2436 - false_positives_5: 11.0000\n",
            "Epoch 127/150\n",
            "168/168 [==============================] - 0s 959us/step - loss: 0.2299 - false_positives_5: 7.0000\n",
            "Epoch 128/150\n",
            "168/168 [==============================] - 0s 980us/step - loss: 0.2174 - false_positives_5: 6.0000\n",
            "Epoch 129/150\n",
            "168/168 [==============================] - 0s 950us/step - loss: 0.2587 - false_positives_5: 13.0000\n",
            "Epoch 130/150\n",
            "168/168 [==============================] - 0s 993us/step - loss: 0.2117 - false_positives_5: 7.0000\n",
            "Epoch 131/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2213 - false_positives_5: 9.0000\n",
            "Epoch 132/150\n",
            "168/168 [==============================] - 0s 972us/step - loss: 0.2271 - false_positives_5: 7.0000\n",
            "Epoch 133/150\n",
            "168/168 [==============================] - 0s 985us/step - loss: 0.2201 - false_positives_5: 10.0000\n",
            "Epoch 134/150\n",
            "168/168 [==============================] - 0s 878us/step - loss: 0.2085 - false_positives_5: 8.0000\n",
            "Epoch 135/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2193 - false_positives_5: 8.0000\n",
            "Epoch 136/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2188 - false_positives_5: 8.0000\n",
            "Epoch 137/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2756 - false_positives_5: 11.0000\n",
            "Epoch 138/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2462 - false_positives_5: 9.0000\n",
            "Epoch 139/150\n",
            "168/168 [==============================] - 0s 983us/step - loss: 0.2319 - false_positives_5: 8.0000\n",
            "Epoch 140/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2274 - false_positives_5: 10.0000\n",
            "Epoch 141/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2164 - false_positives_5: 8.0000\n",
            "Epoch 142/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2131 - false_positives_5: 7.0000\n",
            "Epoch 143/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2338 - false_positives_5: 10.0000\n",
            "Epoch 144/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2271 - false_positives_5: 12.0000\n",
            "Epoch 145/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2142 - false_positives_5: 11.0000\n",
            "Epoch 146/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2398 - false_positives_5: 8.0000\n",
            "Epoch 147/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2077 - false_positives_5: 8.0000\n",
            "Epoch 148/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2378 - false_positives_5: 9.0000\n",
            "Epoch 149/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2319 - false_positives_5: 10.0000\n",
            "Epoch 150/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2545 - false_positives_5: 9.0000\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.4440 - false_positives_5: 5.0000\n",
            "False Positives: 5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLcI_tihJjsu",
        "outputId": "3c6115c7-bc3f-4e40-d84b-6b4e0b4d492c"
      },
      "source": [
        "tf.keras.metrics.FalseNegatives(\n",
        "    thresholds=None, name=None, dtype=None\n",
        ")\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "               metrics=[tf.keras.metrics.FalseNegatives()])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=1)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('False Negatives:', test_acc)\n",
        "FN=test_acc"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "168/168 [==============================] - 0s 970us/step - loss: 0.2452 - false_negatives_1: 7.0000\n",
            "Epoch 2/150\n",
            "168/168 [==============================] - 0s 888us/step - loss: 0.2349 - false_negatives_1: 8.0000\n",
            "Epoch 3/150\n",
            "168/168 [==============================] - 0s 965us/step - loss: 0.2396 - false_negatives_1: 8.0000\n",
            "Epoch 4/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2350 - false_negatives_1: 11.0000\n",
            "Epoch 5/150\n",
            "168/168 [==============================] - 0s 993us/step - loss: 0.2172 - false_negatives_1: 7.0000\n",
            "Epoch 6/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2305 - false_negatives_1: 7.0000\n",
            "Epoch 7/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2180 - false_negatives_1: 7.0000\n",
            "Epoch 8/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2164 - false_negatives_1: 7.0000\n",
            "Epoch 9/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2270 - false_negatives_1: 8.0000\n",
            "Epoch 10/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2215 - false_negatives_1: 7.0000\n",
            "Epoch 11/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2156 - false_negatives_1: 4.0000\n",
            "Epoch 12/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2239 - false_negatives_1: 6.0000\n",
            "Epoch 13/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2171 - false_negatives_1: 7.0000\n",
            "Epoch 14/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2212 - false_negatives_1: 6.0000\n",
            "Epoch 15/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2197 - false_negatives_1: 7.0000\n",
            "Epoch 16/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2143 - false_negatives_1: 8.0000\n",
            "Epoch 17/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2112 - false_negatives_1: 6.0000\n",
            "Epoch 18/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2156 - false_negatives_1: 5.0000\n",
            "Epoch 19/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2200 - false_negatives_1: 7.0000\n",
            "Epoch 20/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2093 - false_negatives_1: 7.0000\n",
            "Epoch 21/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2271 - false_negatives_1: 6.0000\n",
            "Epoch 22/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2251 - false_negatives_1: 7.0000\n",
            "Epoch 23/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2247 - false_negatives_1: 7.0000\n",
            "Epoch 24/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2105 - false_negatives_1: 4.0000\n",
            "Epoch 25/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2542 - false_negatives_1: 8.0000\n",
            "Epoch 26/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2147 - false_negatives_1: 6.0000\n",
            "Epoch 27/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2328 - false_negatives_1: 7.0000\n",
            "Epoch 28/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2116 - false_negatives_1: 6.0000\n",
            "Epoch 29/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2059 - false_negatives_1: 5.0000\n",
            "Epoch 30/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2310 - false_negatives_1: 7.0000\n",
            "Epoch 31/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2058 - false_negatives_1: 6.0000\n",
            "Epoch 32/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1991 - false_negatives_1: 6.0000\n",
            "Epoch 33/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2148 - false_negatives_1: 8.0000\n",
            "Epoch 34/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2448 - false_negatives_1: 8.0000\n",
            "Epoch 35/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2361 - false_negatives_1: 8.0000\n",
            "Epoch 36/150\n",
            "168/168 [==============================] - 0s 946us/step - loss: 0.2153 - false_negatives_1: 5.0000\n",
            "Epoch 37/150\n",
            "168/168 [==============================] - 0s 896us/step - loss: 0.2282 - false_negatives_1: 8.0000\n",
            "Epoch 38/150\n",
            "168/168 [==============================] - 0s 945us/step - loss: 0.2203 - false_negatives_1: 8.0000\n",
            "Epoch 39/150\n",
            "168/168 [==============================] - 0s 960us/step - loss: 0.2145 - false_negatives_1: 8.0000\n",
            "Epoch 40/150\n",
            "168/168 [==============================] - 0s 965us/step - loss: 0.2151 - false_negatives_1: 6.0000\n",
            "Epoch 41/150\n",
            "168/168 [==============================] - 0s 961us/step - loss: 0.2287 - false_negatives_1: 5.0000\n",
            "Epoch 42/150\n",
            "168/168 [==============================] - 0s 891us/step - loss: 0.2171 - false_negatives_1: 7.0000\n",
            "Epoch 43/150\n",
            "168/168 [==============================] - 0s 961us/step - loss: 0.2063 - false_negatives_1: 7.0000\n",
            "Epoch 44/150\n",
            "168/168 [==============================] - 0s 981us/step - loss: 0.2132 - false_negatives_1: 5.0000\n",
            "Epoch 45/150\n",
            "168/168 [==============================] - 0s 897us/step - loss: 0.2160 - false_negatives_1: 8.0000\n",
            "Epoch 46/150\n",
            "168/168 [==============================] - 0s 972us/step - loss: 0.2308 - false_negatives_1: 8.0000\n",
            "Epoch 47/150\n",
            "168/168 [==============================] - 0s 894us/step - loss: 0.2275 - false_negatives_1: 9.0000\n",
            "Epoch 48/150\n",
            "168/168 [==============================] - 0s 941us/step - loss: 0.2131 - false_negatives_1: 6.0000\n",
            "Epoch 49/150\n",
            "168/168 [==============================] - 0s 883us/step - loss: 0.2196 - false_negatives_1: 7.0000\n",
            "Epoch 50/150\n",
            "168/168 [==============================] - 0s 992us/step - loss: 0.2007 - false_negatives_1: 5.0000\n",
            "Epoch 51/150\n",
            "168/168 [==============================] - 0s 926us/step - loss: 0.2201 - false_negatives_1: 6.0000\n",
            "Epoch 52/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2239 - false_negatives_1: 8.0000\n",
            "Epoch 53/150\n",
            "168/168 [==============================] - 0s 898us/step - loss: 0.2192 - false_negatives_1: 6.0000\n",
            "Epoch 54/150\n",
            "168/168 [==============================] - 0s 941us/step - loss: 0.2071 - false_negatives_1: 8.0000\n",
            "Epoch 55/150\n",
            "168/168 [==============================] - 0s 972us/step - loss: 0.2137 - false_negatives_1: 7.0000\n",
            "Epoch 56/150\n",
            "168/168 [==============================] - 0s 981us/step - loss: 0.2141 - false_negatives_1: 7.0000\n",
            "Epoch 57/150\n",
            "168/168 [==============================] - 0s 947us/step - loss: 0.2097 - false_negatives_1: 7.0000\n",
            "Epoch 58/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.2250 - false_negatives_1: 7.0000\n",
            "Epoch 59/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2232 - false_negatives_1: 10.0000\n",
            "Epoch 60/150\n",
            "168/168 [==============================] - 0s 983us/step - loss: 0.2560 - false_negatives_1: 8.0000\n",
            "Epoch 61/150\n",
            "168/168 [==============================] - 0s 908us/step - loss: 0.2128 - false_negatives_1: 9.0000\n",
            "Epoch 62/150\n",
            "168/168 [==============================] - 0s 984us/step - loss: 0.2064 - false_negatives_1: 5.0000\n",
            "Epoch 63/150\n",
            "168/168 [==============================] - 0s 972us/step - loss: 0.2238 - false_negatives_1: 7.0000\n",
            "Epoch 64/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2044 - false_negatives_1: 5.0000\n",
            "Epoch 65/150\n",
            "168/168 [==============================] - 0s 984us/step - loss: 0.2022 - false_negatives_1: 9.0000\n",
            "Epoch 66/150\n",
            "168/168 [==============================] - 0s 973us/step - loss: 0.2239 - false_negatives_1: 6.0000\n",
            "Epoch 67/150\n",
            "168/168 [==============================] - 0s 937us/step - loss: 0.2109 - false_negatives_1: 7.0000\n",
            "Epoch 68/150\n",
            "168/168 [==============================] - 0s 941us/step - loss: 0.2245 - false_negatives_1: 9.0000\n",
            "Epoch 69/150\n",
            "168/168 [==============================] - 0s 984us/step - loss: 0.2034 - false_negatives_1: 8.0000\n",
            "Epoch 70/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1973 - false_negatives_1: 5.0000\n",
            "Epoch 71/150\n",
            "168/168 [==============================] - 0s 875us/step - loss: 0.1866 - false_negatives_1: 7.0000\n",
            "Epoch 72/150\n",
            "168/168 [==============================] - 0s 886us/step - loss: 0.2177 - false_negatives_1: 7.0000\n",
            "Epoch 73/150\n",
            "168/168 [==============================] - 0s 888us/step - loss: 0.2164 - false_negatives_1: 7.0000\n",
            "Epoch 74/150\n",
            "168/168 [==============================] - 0s 971us/step - loss: 0.2065 - false_negatives_1: 10.0000\n",
            "Epoch 75/150\n",
            "168/168 [==============================] - 0s 892us/step - loss: 0.2245 - false_negatives_1: 9.0000\n",
            "Epoch 76/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2116 - false_negatives_1: 8.0000\n",
            "Epoch 77/150\n",
            "168/168 [==============================] - 0s 977us/step - loss: 0.2245 - false_negatives_1: 6.0000\n",
            "Epoch 78/150\n",
            "168/168 [==============================] - 0s 893us/step - loss: 0.2289 - false_negatives_1: 9.0000\n",
            "Epoch 79/150\n",
            "168/168 [==============================] - 0s 948us/step - loss: 0.1958 - false_negatives_1: 6.0000\n",
            "Epoch 80/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2210 - false_negatives_1: 6.0000\n",
            "Epoch 81/150\n",
            "168/168 [==============================] - 0s 954us/step - loss: 0.1971 - false_negatives_1: 5.0000\n",
            "Epoch 82/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.1972 - false_negatives_1: 6.0000\n",
            "Epoch 83/150\n",
            "168/168 [==============================] - 0s 912us/step - loss: 0.2199 - false_negatives_1: 6.0000\n",
            "Epoch 84/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2044 - false_negatives_1: 8.0000\n",
            "Epoch 85/150\n",
            "168/168 [==============================] - 0s 986us/step - loss: 0.2248 - false_negatives_1: 9.0000\n",
            "Epoch 86/150\n",
            "168/168 [==============================] - 0s 957us/step - loss: 0.2134 - false_negatives_1: 5.0000\n",
            "Epoch 87/150\n",
            "168/168 [==============================] - 0s 950us/step - loss: 0.2053 - false_negatives_1: 9.0000\n",
            "Epoch 88/150\n",
            "168/168 [==============================] - 0s 983us/step - loss: 0.2051 - false_negatives_1: 6.0000\n",
            "Epoch 89/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2044 - false_negatives_1: 5.0000\n",
            "Epoch 90/150\n",
            "168/168 [==============================] - 0s 911us/step - loss: 0.2139 - false_negatives_1: 5.0000\n",
            "Epoch 91/150\n",
            "168/168 [==============================] - 0s 979us/step - loss: 0.2353 - false_negatives_1: 8.0000\n",
            "Epoch 92/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2113 - false_negatives_1: 6.0000\n",
            "Epoch 93/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2081 - false_negatives_1: 8.0000\n",
            "Epoch 94/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2098 - false_negatives_1: 8.0000\n",
            "Epoch 95/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1961 - false_negatives_1: 5.0000\n",
            "Epoch 96/150\n",
            "168/168 [==============================] - 0s 982us/step - loss: 0.2016 - false_negatives_1: 9.0000\n",
            "Epoch 97/150\n",
            "168/168 [==============================] - 0s 925us/step - loss: 0.2177 - false_negatives_1: 8.0000\n",
            "Epoch 98/150\n",
            "168/168 [==============================] - 0s 954us/step - loss: 0.2070 - false_negatives_1: 7.0000\n",
            "Epoch 99/150\n",
            "168/168 [==============================] - 0s 963us/step - loss: 0.1986 - false_negatives_1: 6.0000\n",
            "Epoch 100/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2020 - false_negatives_1: 7.0000\n",
            "Epoch 101/150\n",
            "168/168 [==============================] - 0s 894us/step - loss: 0.2212 - false_negatives_1: 7.0000\n",
            "Epoch 102/150\n",
            "168/168 [==============================] - 0s 912us/step - loss: 0.2180 - false_negatives_1: 8.0000\n",
            "Epoch 103/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2217 - false_negatives_1: 9.0000\n",
            "Epoch 104/150\n",
            "168/168 [==============================] - 0s 894us/step - loss: 0.1978 - false_negatives_1: 6.0000\n",
            "Epoch 105/150\n",
            "168/168 [==============================] - 0s 965us/step - loss: 0.2019 - false_negatives_1: 7.0000\n",
            "Epoch 106/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2412 - false_negatives_1: 11.0000\n",
            "Epoch 107/150\n",
            "168/168 [==============================] - 0s 976us/step - loss: 0.2266 - false_negatives_1: 8.0000\n",
            "Epoch 108/150\n",
            "168/168 [==============================] - 0s 942us/step - loss: 0.2125 - false_negatives_1: 6.0000\n",
            "Epoch 109/150\n",
            "168/168 [==============================] - 0s 964us/step - loss: 0.2128 - false_negatives_1: 7.0000\n",
            "Epoch 110/150\n",
            "168/168 [==============================] - 0s 979us/step - loss: 0.2070 - false_negatives_1: 7.0000\n",
            "Epoch 111/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2053 - false_negatives_1: 7.0000\n",
            "Epoch 112/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2085 - false_negatives_1: 6.0000\n",
            "Epoch 113/150\n",
            "168/168 [==============================] - 0s 985us/step - loss: 0.2226 - false_negatives_1: 6.0000\n",
            "Epoch 114/150\n",
            "168/168 [==============================] - 0s 929us/step - loss: 0.1931 - false_negatives_1: 6.0000\n",
            "Epoch 115/150\n",
            "168/168 [==============================] - 0s 935us/step - loss: 0.2235 - false_negatives_1: 9.0000\n",
            "Epoch 116/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2117 - false_negatives_1: 8.0000\n",
            "Epoch 117/150\n",
            "168/168 [==============================] - 0s 951us/step - loss: 0.2033 - false_negatives_1: 6.0000\n",
            "Epoch 118/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2139 - false_negatives_1: 7.0000\n",
            "Epoch 119/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2002 - false_negatives_1: 8.0000\n",
            "Epoch 120/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2297 - false_negatives_1: 9.0000\n",
            "Epoch 121/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2120 - false_negatives_1: 8.0000\n",
            "Epoch 122/150\n",
            "168/168 [==============================] - 0s 966us/step - loss: 0.1841 - false_negatives_1: 3.0000\n",
            "Epoch 123/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2355 - false_negatives_1: 9.0000\n",
            "Epoch 124/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2046 - false_negatives_1: 7.0000\n",
            "Epoch 125/150\n",
            "168/168 [==============================] - 0s 957us/step - loss: 0.2044 - false_negatives_1: 6.0000\n",
            "Epoch 126/150\n",
            "168/168 [==============================] - 0s 987us/step - loss: 0.2023 - false_negatives_1: 8.0000\n",
            "Epoch 127/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2249 - false_negatives_1: 8.0000\n",
            "Epoch 128/150\n",
            "168/168 [==============================] - 0s 973us/step - loss: 0.1896 - false_negatives_1: 6.0000\n",
            "Epoch 129/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2061 - false_negatives_1: 7.0000\n",
            "Epoch 130/150\n",
            "168/168 [==============================] - 0s 930us/step - loss: 0.2316 - false_negatives_1: 9.0000\n",
            "Epoch 131/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2414 - false_negatives_1: 7.0000\n",
            "Epoch 132/150\n",
            "168/168 [==============================] - 0s 939us/step - loss: 0.2174 - false_negatives_1: 6.0000\n",
            "Epoch 133/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1912 - false_negatives_1: 5.0000\n",
            "Epoch 134/150\n",
            "168/168 [==============================] - 0s 950us/step - loss: 0.2145 - false_negatives_1: 7.0000\n",
            "Epoch 135/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2128 - false_negatives_1: 7.0000\n",
            "Epoch 136/150\n",
            "168/168 [==============================] - 0s 942us/step - loss: 0.2137 - false_negatives_1: 8.0000\n",
            "Epoch 137/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.2021 - false_negatives_1: 8.0000\n",
            "Epoch 138/150\n",
            "168/168 [==============================] - 0s 964us/step - loss: 0.1814 - false_negatives_1: 5.0000\n",
            "Epoch 139/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2436 - false_negatives_1: 10.0000\n",
            "Epoch 140/150\n",
            "168/168 [==============================] - 0s 942us/step - loss: 0.2015 - false_negatives_1: 6.0000\n",
            "Epoch 141/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1965 - false_negatives_1: 6.0000\n",
            "Epoch 142/150\n",
            "168/168 [==============================] - 0s 979us/step - loss: 0.1966 - false_negatives_1: 7.0000\n",
            "Epoch 143/150\n",
            "168/168 [==============================] - 0s 934us/step - loss: 0.1913 - false_negatives_1: 4.0000\n",
            "Epoch 144/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1874 - false_negatives_1: 5.0000\n",
            "Epoch 145/150\n",
            "168/168 [==============================] - 0s 909us/step - loss: 0.2145 - false_negatives_1: 6.0000\n",
            "Epoch 146/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2141 - false_negatives_1: 9.0000\n",
            "Epoch 147/150\n",
            "168/168 [==============================] - 0s 998us/step - loss: 0.1860 - false_negatives_1: 7.0000\n",
            "Epoch 148/150\n",
            "168/168 [==============================] - 0s 991us/step - loss: 0.1895 - false_negatives_1: 5.0000\n",
            "Epoch 149/150\n",
            "168/168 [==============================] - 0s 922us/step - loss: 0.2031 - false_negatives_1: 7.0000\n",
            "Epoch 150/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2171 - false_negatives_1: 8.0000\n",
            "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_test_function.<locals>.test_function at 0x7fdb3a168048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 2ms/step - loss: 0.4874 - false_negatives_1: 3.0000\n",
            "False Negatives: 3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R60l8EdEK3Yp",
        "outputId": "89a548df-a38f-4ecb-d00e-159dea5309c8"
      },
      "source": [
        "tf.keras.metrics.TrueNegatives(\n",
        "    thresholds=None, name=None, dtype=None\n",
        ")\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "               metrics=[tf.keras.metrics.TrueNegatives()])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=1)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('True Negatives:', test_acc)\n",
        "TN=test_acc"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "168/168 [==============================] - 0s 881us/step - loss: 0.2394 - true_negatives_1: 72.0000\n",
            "Epoch 2/150\n",
            "168/168 [==============================] - 0s 964us/step - loss: 0.2036 - true_negatives_1: 75.0000\n",
            "Epoch 3/150\n",
            "168/168 [==============================] - 0s 873us/step - loss: 0.2049 - true_negatives_1: 76.0000\n",
            "Epoch 4/150\n",
            "168/168 [==============================] - 0s 925us/step - loss: 0.1961 - true_negatives_1: 76.0000\n",
            "Epoch 5/150\n",
            "168/168 [==============================] - 0s 892us/step - loss: 0.1979 - true_negatives_1: 74.0000\n",
            "Epoch 6/150\n",
            "168/168 [==============================] - 0s 892us/step - loss: 0.2194 - true_negatives_1: 74.0000\n",
            "Epoch 7/150\n",
            "168/168 [==============================] - 0s 883us/step - loss: 0.1934 - true_negatives_1: 74.0000\n",
            "Epoch 8/150\n",
            "168/168 [==============================] - 0s 909us/step - loss: 0.1997 - true_negatives_1: 73.0000\n",
            "Epoch 9/150\n",
            "168/168 [==============================] - 0s 942us/step - loss: 0.1929 - true_negatives_1: 76.0000\n",
            "Epoch 10/150\n",
            "168/168 [==============================] - 0s 890us/step - loss: 0.2206 - true_negatives_1: 73.0000\n",
            "Epoch 11/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1936 - true_negatives_1: 76.0000\n",
            "Epoch 12/150\n",
            "168/168 [==============================] - 0s 882us/step - loss: 0.1975 - true_negatives_1: 75.0000\n",
            "Epoch 13/150\n",
            "168/168 [==============================] - 0s 900us/step - loss: 0.2071 - true_negatives_1: 76.0000\n",
            "Epoch 14/150\n",
            "168/168 [==============================] - 0s 988us/step - loss: 0.1983 - true_negatives_1: 72.0000\n",
            "Epoch 15/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2133 - true_negatives_1: 75.0000\n",
            "Epoch 16/150\n",
            "168/168 [==============================] - 0s 958us/step - loss: 0.1969 - true_negatives_1: 74.0000\n",
            "Epoch 17/150\n",
            "168/168 [==============================] - 0s 951us/step - loss: 0.2176 - true_negatives_1: 75.0000\n",
            "Epoch 18/150\n",
            "168/168 [==============================] - 0s 864us/step - loss: 0.2328 - true_negatives_1: 72.0000\n",
            "Epoch 19/150\n",
            "168/168 [==============================] - 0s 885us/step - loss: 0.2027 - true_negatives_1: 75.0000\n",
            "Epoch 20/150\n",
            "168/168 [==============================] - 0s 941us/step - loss: 0.1907 - true_negatives_1: 73.0000\n",
            "Epoch 21/150\n",
            "168/168 [==============================] - 0s 953us/step - loss: 0.1920 - true_negatives_1: 73.0000\n",
            "Epoch 22/150\n",
            "168/168 [==============================] - 0s 906us/step - loss: 0.2058 - true_negatives_1: 73.0000\n",
            "Epoch 23/150\n",
            "168/168 [==============================] - 0s 930us/step - loss: 0.2090 - true_negatives_1: 75.0000\n",
            "Epoch 24/150\n",
            "168/168 [==============================] - 0s 867us/step - loss: 0.1901 - true_negatives_1: 75.0000\n",
            "Epoch 25/150\n",
            "168/168 [==============================] - 0s 884us/step - loss: 0.1962 - true_negatives_1: 77.0000\n",
            "Epoch 26/150\n",
            "168/168 [==============================] - 0s 892us/step - loss: 0.1977 - true_negatives_1: 75.0000\n",
            "Epoch 27/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2010 - true_negatives_1: 74.0000\n",
            "Epoch 28/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1951 - true_negatives_1: 74.0000\n",
            "Epoch 29/150\n",
            "168/168 [==============================] - 0s 879us/step - loss: 0.1983 - true_negatives_1: 75.0000\n",
            "Epoch 30/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1970 - true_negatives_1: 75.0000\n",
            "Epoch 31/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1995 - true_negatives_1: 74.0000\n",
            "Epoch 32/150\n",
            "168/168 [==============================] - 0s 989us/step - loss: 0.2095 - true_negatives_1: 76.0000\n",
            "Epoch 33/150\n",
            "168/168 [==============================] - 0s 952us/step - loss: 0.2476 - true_negatives_1: 75.0000\n",
            "Epoch 34/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2194 - true_negatives_1: 70.0000\n",
            "Epoch 35/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1987 - true_negatives_1: 74.0000\n",
            "Epoch 36/150\n",
            "168/168 [==============================] - 0s 940us/step - loss: 0.1872 - true_negatives_1: 76.0000\n",
            "Epoch 37/150\n",
            "168/168 [==============================] - 0s 990us/step - loss: 0.1932 - true_negatives_1: 74.0000\n",
            "Epoch 38/150\n",
            "168/168 [==============================] - 0s 987us/step - loss: 0.1988 - true_negatives_1: 75.0000\n",
            "Epoch 39/150\n",
            "168/168 [==============================] - 0s 926us/step - loss: 0.1995 - true_negatives_1: 75.0000\n",
            "Epoch 40/150\n",
            "168/168 [==============================] - 0s 987us/step - loss: 0.2007 - true_negatives_1: 75.0000\n",
            "Epoch 41/150\n",
            "168/168 [==============================] - 0s 988us/step - loss: 0.1968 - true_negatives_1: 76.0000\n",
            "Epoch 42/150\n",
            "168/168 [==============================] - 0s 966us/step - loss: 0.2348 - true_negatives_1: 75.0000\n",
            "Epoch 43/150\n",
            "168/168 [==============================] - 0s 934us/step - loss: 0.1933 - true_negatives_1: 74.0000\n",
            "Epoch 44/150\n",
            "168/168 [==============================] - 0s 975us/step - loss: 0.1991 - true_negatives_1: 74.0000\n",
            "Epoch 45/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.1981 - true_negatives_1: 73.0000\n",
            "Epoch 46/150\n",
            "168/168 [==============================] - 0s 942us/step - loss: 0.1989 - true_negatives_1: 77.0000\n",
            "Epoch 47/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2244 - true_negatives_1: 76.0000\n",
            "Epoch 48/150\n",
            "168/168 [==============================] - 0s 965us/step - loss: 0.2051 - true_negatives_1: 77.0000\n",
            "Epoch 49/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2000 - true_negatives_1: 75.0000\n",
            "Epoch 50/150\n",
            "168/168 [==============================] - 0s 931us/step - loss: 0.1859 - true_negatives_1: 77.0000\n",
            "Epoch 51/150\n",
            "168/168 [==============================] - 0s 986us/step - loss: 0.1964 - true_negatives_1: 75.0000\n",
            "Epoch 52/150\n",
            "168/168 [==============================] - 0s 978us/step - loss: 0.1848 - true_negatives_1: 74.0000\n",
            "Epoch 53/150\n",
            "168/168 [==============================] - 0s 966us/step - loss: 0.2024 - true_negatives_1: 75.0000\n",
            "Epoch 54/150\n",
            "168/168 [==============================] - 0s 964us/step - loss: 0.2113 - true_negatives_1: 76.0000\n",
            "Epoch 55/150\n",
            "168/168 [==============================] - 0s 984us/step - loss: 0.2278 - true_negatives_1: 73.0000\n",
            "Epoch 56/150\n",
            "168/168 [==============================] - 0s 966us/step - loss: 0.1928 - true_negatives_1: 73.0000\n",
            "Epoch 57/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1939 - true_negatives_1: 76.0000\n",
            "Epoch 58/150\n",
            "168/168 [==============================] - 0s 911us/step - loss: 0.1969 - true_negatives_1: 78.0000\n",
            "Epoch 59/150\n",
            "168/168 [==============================] - 0s 983us/step - loss: 0.1927 - true_negatives_1: 75.0000\n",
            "Epoch 60/150\n",
            "168/168 [==============================] - 0s 932us/step - loss: 0.2051 - true_negatives_1: 74.0000\n",
            "Epoch 61/150\n",
            "168/168 [==============================] - 0s 968us/step - loss: 0.2174 - true_negatives_1: 75.0000\n",
            "Epoch 62/150\n",
            "168/168 [==============================] - 0s 957us/step - loss: 0.2021 - true_negatives_1: 75.0000\n",
            "Epoch 63/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.1983 - true_negatives_1: 75.0000\n",
            "Epoch 64/150\n",
            "168/168 [==============================] - 0s 927us/step - loss: 0.1920 - true_negatives_1: 74.0000\n",
            "Epoch 65/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1634 - true_negatives_1: 76.0000\n",
            "Epoch 66/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2674 - true_negatives_1: 73.0000\n",
            "Epoch 67/150\n",
            "168/168 [==============================] - 0s 976us/step - loss: 0.1914 - true_negatives_1: 76.0000\n",
            "Epoch 68/150\n",
            "168/168 [==============================] - 0s 960us/step - loss: 0.1873 - true_negatives_1: 76.0000\n",
            "Epoch 69/150\n",
            "168/168 [==============================] - 0s 967us/step - loss: 0.1966 - true_negatives_1: 72.0000\n",
            "Epoch 70/150\n",
            "168/168 [==============================] - 0s 964us/step - loss: 0.2224 - true_negatives_1: 76.0000\n",
            "Epoch 71/150\n",
            "168/168 [==============================] - 0s 981us/step - loss: 0.1747 - true_negatives_1: 76.0000\n",
            "Epoch 72/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2030 - true_negatives_1: 74.0000\n",
            "Epoch 73/150\n",
            "168/168 [==============================] - 0s 962us/step - loss: 0.1832 - true_negatives_1: 77.0000\n",
            "Epoch 74/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1845 - true_negatives_1: 76.0000\n",
            "Epoch 75/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2240 - true_negatives_1: 72.0000\n",
            "Epoch 76/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2007 - true_negatives_1: 75.0000\n",
            "Epoch 77/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1883 - true_negatives_1: 75.0000\n",
            "Epoch 78/150\n",
            "168/168 [==============================] - 0s 904us/step - loss: 0.2044 - true_negatives_1: 74.0000\n",
            "Epoch 79/150\n",
            "168/168 [==============================] - 0s 989us/step - loss: 0.1893 - true_negatives_1: 74.0000\n",
            "Epoch 80/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1993 - true_negatives_1: 77.0000\n",
            "Epoch 81/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1839 - true_negatives_1: 77.0000\n",
            "Epoch 82/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1909 - true_negatives_1: 75.0000\n",
            "Epoch 83/150\n",
            "168/168 [==============================] - 0s 939us/step - loss: 0.2285 - true_negatives_1: 75.0000\n",
            "Epoch 84/150\n",
            "168/168 [==============================] - 0s 977us/step - loss: 0.1881 - true_negatives_1: 75.0000\n",
            "Epoch 85/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2256 - true_negatives_1: 76.0000\n",
            "Epoch 86/150\n",
            "168/168 [==============================] - 0s 947us/step - loss: 0.2221 - true_negatives_1: 76.0000\n",
            "Epoch 87/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1958 - true_negatives_1: 75.0000\n",
            "Epoch 88/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1787 - true_negatives_1: 75.0000\n",
            "Epoch 89/150\n",
            "168/168 [==============================] - 0s 996us/step - loss: 0.1900 - true_negatives_1: 75.0000\n",
            "Epoch 90/150\n",
            "168/168 [==============================] - 0s 956us/step - loss: 0.2152 - true_negatives_1: 76.0000\n",
            "Epoch 91/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.1822 - true_negatives_1: 77.0000\n",
            "Epoch 92/150\n",
            "168/168 [==============================] - 0s 974us/step - loss: 0.1842 - true_negatives_1: 78.0000\n",
            "Epoch 93/150\n",
            "168/168 [==============================] - 0s 994us/step - loss: 0.1772 - true_negatives_1: 78.0000\n",
            "Epoch 94/150\n",
            "168/168 [==============================] - 0s 940us/step - loss: 0.1976 - true_negatives_1: 75.0000\n",
            "Epoch 95/150\n",
            "168/168 [==============================] - 0s 997us/step - loss: 0.1780 - true_negatives_1: 76.0000\n",
            "Epoch 96/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2014 - true_negatives_1: 75.0000\n",
            "Epoch 97/150\n",
            "168/168 [==============================] - 0s 951us/step - loss: 0.1739 - true_negatives_1: 77.0000\n",
            "Epoch 98/150\n",
            "168/168 [==============================] - 0s 978us/step - loss: 0.2080 - true_negatives_1: 76.0000\n",
            "Epoch 99/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2040 - true_negatives_1: 73.0000\n",
            "Epoch 100/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2122 - true_negatives_1: 77.0000\n",
            "Epoch 101/150\n",
            "168/168 [==============================] - 0s 928us/step - loss: 0.1902 - true_negatives_1: 73.0000\n",
            "Epoch 102/150\n",
            "168/168 [==============================] - 0s 983us/step - loss: 0.1937 - true_negatives_1: 74.0000\n",
            "Epoch 103/150\n",
            "168/168 [==============================] - 0s 987us/step - loss: 0.1925 - true_negatives_1: 73.0000\n",
            "Epoch 104/150\n",
            "168/168 [==============================] - 0s 914us/step - loss: 0.1936 - true_negatives_1: 75.0000\n",
            "Epoch 105/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1887 - true_negatives_1: 77.0000\n",
            "Epoch 106/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1825 - true_negatives_1: 77.0000\n",
            "Epoch 107/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1987 - true_negatives_1: 75.0000\n",
            "Epoch 108/150\n",
            "168/168 [==============================] - 0s 986us/step - loss: 0.2170 - true_negatives_1: 77.0000\n",
            "Epoch 109/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2057 - true_negatives_1: 73.0000\n",
            "Epoch 110/150\n",
            "168/168 [==============================] - 0s 984us/step - loss: 0.2297 - true_negatives_1: 72.0000\n",
            "Epoch 111/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1938 - true_negatives_1: 76.0000\n",
            "Epoch 112/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2055 - true_negatives_1: 76.0000\n",
            "Epoch 113/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1896 - true_negatives_1: 76.0000\n",
            "Epoch 114/150\n",
            "168/168 [==============================] - 0s 998us/step - loss: 0.1841 - true_negatives_1: 74.0000\n",
            "Epoch 115/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1850 - true_negatives_1: 75.0000\n",
            "Epoch 116/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1998 - true_negatives_1: 75.0000\n",
            "Epoch 117/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1959 - true_negatives_1: 77.0000\n",
            "Epoch 118/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1765 - true_negatives_1: 76.0000\n",
            "Epoch 119/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1953 - true_negatives_1: 76.0000\n",
            "Epoch 120/150\n",
            "168/168 [==============================] - 0s 956us/step - loss: 0.1873 - true_negatives_1: 76.0000\n",
            "Epoch 121/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2293 - true_negatives_1: 73.0000\n",
            "Epoch 122/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1825 - true_negatives_1: 76.0000\n",
            "Epoch 123/150\n",
            "168/168 [==============================] - 0s 960us/step - loss: 0.2088 - true_negatives_1: 74.0000\n",
            "Epoch 124/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1924 - true_negatives_1: 75.0000\n",
            "Epoch 125/150\n",
            "168/168 [==============================] - 0s 971us/step - loss: 0.1752 - true_negatives_1: 76.0000\n",
            "Epoch 126/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1968 - true_negatives_1: 75.0000\n",
            "Epoch 127/150\n",
            "168/168 [==============================] - 0s 975us/step - loss: 0.1849 - true_negatives_1: 74.0000\n",
            "Epoch 128/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1855 - true_negatives_1: 76.0000\n",
            "Epoch 129/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1923 - true_negatives_1: 77.0000\n",
            "Epoch 130/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1953 - true_negatives_1: 73.0000\n",
            "Epoch 131/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1898 - true_negatives_1: 75.0000\n",
            "Epoch 132/150\n",
            "168/168 [==============================] - 0s 934us/step - loss: 0.1923 - true_negatives_1: 76.0000\n",
            "Epoch 133/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2158 - true_negatives_1: 72.0000\n",
            "Epoch 134/150\n",
            "168/168 [==============================] - 0s 998us/step - loss: 0.1754 - true_negatives_1: 76.0000\n",
            "Epoch 135/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1785 - true_negatives_1: 76.0000\n",
            "Epoch 136/150\n",
            "168/168 [==============================] - 0s 910us/step - loss: 0.1892 - true_negatives_1: 75.0000\n",
            "Epoch 137/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1730 - true_negatives_1: 75.0000\n",
            "Epoch 138/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1952 - true_negatives_1: 77.0000\n",
            "Epoch 139/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1854 - true_negatives_1: 77.0000\n",
            "Epoch 140/150\n",
            "168/168 [==============================] - 0s 993us/step - loss: 0.1981 - true_negatives_1: 76.0000\n",
            "Epoch 141/150\n",
            "168/168 [==============================] - 0s 916us/step - loss: 0.1904 - true_negatives_1: 75.0000\n",
            "Epoch 142/150\n",
            "168/168 [==============================] - 0s 999us/step - loss: 0.1974 - true_negatives_1: 74.0000\n",
            "Epoch 143/150\n",
            "168/168 [==============================] - 0s 900us/step - loss: 0.1882 - true_negatives_1: 76.0000\n",
            "Epoch 144/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1995 - true_negatives_1: 76.0000\n",
            "Epoch 145/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1982 - true_negatives_1: 74.0000\n",
            "Epoch 146/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.1944 - true_negatives_1: 74.0000\n",
            "Epoch 147/150\n",
            "168/168 [==============================] - 0s 956us/step - loss: 0.1917 - true_negatives_1: 77.0000\n",
            "Epoch 148/150\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.2045 - true_negatives_1: 72.0000\n",
            "Epoch 149/150\n",
            "168/168 [==============================] - 0s 982us/step - loss: 0.1874 - true_negatives_1: 76.0000\n",
            "Epoch 150/150\n",
            "168/168 [==============================] - 0s 984us/step - loss: 0.2111 - true_negatives_1: 77.0000\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fdb3505bd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5841 - true_negatives_1: 28.0000\n",
            "True Negatives: 28.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df4I5_rwLLCx",
        "outputId": "df42e6ad-af9d-4426-c6b5-367396fad247"
      },
      "source": [
        "tf.keras.metrics.TruePositives(\n",
        "    thresholds=None, name=None, dtype=None\n",
        ")\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "               metrics=[tf.keras.metrics.TruePositives()])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=1)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('True Positives:', test_acc)\n",
        "TP=test_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "168/168 [==============================] - 0s 895us/step - loss: 0.3102 - true_positives_2: 81.0000\n",
            "Epoch 2/50\n",
            "168/168 [==============================] - 0s 950us/step - loss: 0.3176 - true_positives_2: 82.0000\n",
            "Epoch 3/50\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3159 - true_positives_2: 81.0000\n",
            "Epoch 4/50\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3127 - true_positives_2: 80.0000\n",
            "Epoch 5/50\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3084 - true_positives_2: 83.0000\n",
            "Epoch 6/50\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3239 - true_positives_2: 79.0000\n",
            "Epoch 7/50\n",
            "168/168 [==============================] - 0s 961us/step - loss: 0.3093 - true_positives_2: 79.0000\n",
            "Epoch 8/50\n",
            "168/168 [==============================] - 0s 984us/step - loss: 0.3090 - true_positives_2: 79.0000\n",
            "Epoch 9/50\n",
            "168/168 [==============================] - 0s 970us/step - loss: 0.3093 - true_positives_2: 83.0000\n",
            "Epoch 10/50\n",
            "168/168 [==============================] - 0s 878us/step - loss: 0.3108 - true_positives_2: 80.0000\n",
            "Epoch 11/50\n",
            "168/168 [==============================] - 0s 895us/step - loss: 0.3198 - true_positives_2: 81.0000\n",
            "Epoch 12/50\n",
            "168/168 [==============================] - 0s 882us/step - loss: 0.3144 - true_positives_2: 77.0000\n",
            "Epoch 13/50\n",
            "168/168 [==============================] - 0s 950us/step - loss: 0.3190 - true_positives_2: 82.0000\n",
            "Epoch 14/50\n",
            "168/168 [==============================] - 0s 963us/step - loss: 0.3076 - true_positives_2: 80.0000\n",
            "Epoch 15/50\n",
            "168/168 [==============================] - 0s 898us/step - loss: 0.3189 - true_positives_2: 80.0000\n",
            "Epoch 16/50\n",
            "168/168 [==============================] - 0s 938us/step - loss: 0.3153 - true_positives_2: 82.0000\n",
            "Epoch 17/50\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3031 - true_positives_2: 78.0000\n",
            "Epoch 18/50\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3256 - true_positives_2: 81.0000\n",
            "Epoch 19/50\n",
            "168/168 [==============================] - 0s 942us/step - loss: 0.3167 - true_positives_2: 78.0000\n",
            "Epoch 20/50\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3062 - true_positives_2: 80.0000\n",
            "Epoch 21/50\n",
            "168/168 [==============================] - 0s 899us/step - loss: 0.3114 - true_positives_2: 81.0000\n",
            "Epoch 22/50\n",
            "168/168 [==============================] - 0s 848us/step - loss: 0.3181 - true_positives_2: 80.0000\n",
            "Epoch 23/50\n",
            "168/168 [==============================] - 0s 877us/step - loss: 0.3142 - true_positives_2: 79.0000\n",
            "Epoch 24/50\n",
            "168/168 [==============================] - 0s 990us/step - loss: 0.3135 - true_positives_2: 78.0000\n",
            "Epoch 25/50\n",
            "168/168 [==============================] - 0s 908us/step - loss: 0.3053 - true_positives_2: 78.0000\n",
            "Epoch 26/50\n",
            "168/168 [==============================] - 0s 1ms/step - loss: 0.3086 - true_positives_2: 80.0000\n",
            "Epoch 27/50\n",
            "168/168 [==============================] - 0s 892us/step - loss: 0.3300 - true_positives_2: 78.0000\n",
            "Epoch 28/50\n",
            "168/168 [==============================] - 0s 942us/step - loss: 0.3178 - true_positives_2: 84.0000\n",
            "Epoch 29/50\n",
            "168/168 [==============================] - 0s 868us/step - loss: 0.3207 - true_positives_2: 77.0000\n",
            "Epoch 30/50\n",
            "168/168 [==============================] - 0s 896us/step - loss: 0.3070 - true_positives_2: 79.0000\n",
            "Epoch 31/50\n",
            "168/168 [==============================] - 0s 884us/step - loss: 0.3082 - true_positives_2: 80.0000\n",
            "Epoch 32/50\n",
            "168/168 [==============================] - 0s 957us/step - loss: 0.3184 - true_positives_2: 79.0000\n",
            "Epoch 33/50\n",
            "168/168 [==============================] - 0s 962us/step - loss: 0.3132 - true_positives_2: 78.0000\n",
            "Epoch 34/50\n",
            "168/168 [==============================] - 0s 891us/step - loss: 0.3103 - true_positives_2: 83.0000\n",
            "Epoch 35/50\n",
            "168/168 [==============================] - 0s 875us/step - loss: 0.3041 - true_positives_2: 81.0000\n",
            "Epoch 36/50\n",
            "168/168 [==============================] - 0s 897us/step - loss: 0.3112 - true_positives_2: 80.0000\n",
            "Epoch 37/50\n",
            "168/168 [==============================] - 0s 961us/step - loss: 0.3232 - true_positives_2: 79.0000\n",
            "Epoch 38/50\n",
            "168/168 [==============================] - 0s 939us/step - loss: 0.3087 - true_positives_2: 81.0000\n",
            "Epoch 39/50\n",
            "168/168 [==============================] - 0s 985us/step - loss: 0.3060 - true_positives_2: 83.0000\n",
            "Epoch 40/50\n",
            "168/168 [==============================] - 0s 885us/step - loss: 0.3126 - true_positives_2: 77.0000\n",
            "Epoch 41/50\n",
            "168/168 [==============================] - 0s 931us/step - loss: 0.2985 - true_positives_2: 82.0000\n",
            "Epoch 42/50\n",
            "168/168 [==============================] - 0s 939us/step - loss: 0.3112 - true_positives_2: 79.0000\n",
            "Epoch 43/50\n",
            "168/168 [==============================] - 0s 995us/step - loss: 0.3035 - true_positives_2: 81.0000\n",
            "Epoch 44/50\n",
            "168/168 [==============================] - 0s 904us/step - loss: 0.3103 - true_positives_2: 82.0000\n",
            "Epoch 45/50\n",
            "168/168 [==============================] - 0s 988us/step - loss: 0.3125 - true_positives_2: 80.0000\n",
            "Epoch 46/50\n",
            "168/168 [==============================] - 0s 944us/step - loss: 0.3128 - true_positives_2: 77.0000\n",
            "Epoch 47/50\n",
            "168/168 [==============================] - 0s 911us/step - loss: 0.3164 - true_positives_2: 81.0000\n",
            "Epoch 48/50\n",
            "168/168 [==============================] - 0s 937us/step - loss: 0.3052 - true_positives_2: 83.0000\n",
            "Epoch 49/50\n",
            "168/168 [==============================] - 0s 881us/step - loss: 0.3038 - true_positives_2: 78.0000\n",
            "Epoch 50/50\n",
            "168/168 [==============================] - 0s 954us/step - loss: 0.3022 - true_positives_2: 78.0000\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff3fc2aee18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.3700 - true_positives_2: 37.0000\n",
            "Test accuracy: 37.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHwOH3ZyL99G",
        "outputId": "aa4c7939-69e2-4c8e-9400-4efb6c9f1697"
      },
      "source": [
        "import math  \n",
        "accuracy=(TP+TN)/(TP+TN+FP+FN)\n",
        "sensitivity=TP/(TP+FN)\n",
        "specifisity=TN/(TN+FP)\n",
        "mcc=((TP*TN)-(FP*FN))/(math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)))\n",
        "f1=TP/(TP+0.5*(FP+FN))\n",
        "print(TP,TN,FP,FN)\n",
        "print(\"Accuracy\",accuracy, sep=\" : \")  \n",
        "print(\"Sensitivity\",sensitivity, sep=\" : \")  \n",
        "print(\"Specifisity\",specifisity, sep=\" : \")  \n",
        "print(\"F-1 Score\",f1, sep=\" : \") \n",
        "print(\"MCC\",mcc, sep=\" : \") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37 25 2 8\n",
            "Accuracy : 0.8611111111111112\n",
            "Sensitivity : 0.8222222222222222\n",
            "Specifisity : 0.9259259259259259\n",
            "F-1 Score : 0.8809523809523809\n",
            "MCC : 0.7269197537640096\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}